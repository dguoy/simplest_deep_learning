{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAU = 0.01\n",
    "GAMMA = 0.9\n",
    "MAX_EP_STEPS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "env = env.unwrapped\n",
    "\n",
    "print(env.action_space)\n",
    "print(env.action_space.high)\n",
    "print(env.action_space.low)\n",
    "print(env.observation_space)\n",
    "print(env.observation_space.high)\n",
    "print(env.observation_space.low)\n",
    "\n",
    "state_space_dim = env.observation_space.shape[0]\n",
    "action_space_dim = env.action_space.shape[0]\n",
    "action_space_bound = env.action_space.high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_inputs = tf.placeholder(tf.float32, shape=[None, state_space_dim], name='states')\n",
    "reward_inputs = tf.placeholder(tf.float32, [None], name='rewards')\n",
    "next_state_inputs = tf.placeholder(tf.float32, shape=[None, state_space_dim], name='next_states')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('actor'):\n",
    "    hidden = tf.layers.dense(state_inputs, 32, activation=tf.nn.relu)\n",
    "    unscaled_actions = tf.layers.dense(hidden, action_space_dim, activation=tf.nn.tanh)\n",
    "    actions = action_space_bound * unscaled_actions\n",
    "\n",
    "with tf.variable_scope('target_actor'):\n",
    "    hidden = tf.layers.dense(next_state_inputs, 32, activation=tf.nn.relu, trainable=False)\n",
    "    unscaled_actions = tf.layers.dense(hidden, action_space_dim, activation=tf.nn.tanh, trainable=False)\n",
    "    target_actions = action_space_bound * unscaled_actions\n",
    "\n",
    "actor_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='actor')\n",
    "actor_target_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='target_actor')\n",
    "update_actor_target_op = [tf.assign(atp, (1 - TAU) * atp + TAU * ap) for atp, ap in zip(actor_target_params, actor_params)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('critic'):\n",
    "    hidden = tf.layers.dense(\n",
    "        tf.concat([state_inputs, actions], axis=-1), 32, activation=tf.nn.relu)\n",
    "    q_values = tf.squeeze(tf.layers.dense(hidden, 1, activation=None), axis=-1)\n",
    "\n",
    "with tf.variable_scope('target_critic'):\n",
    "    hidden = tf.layers.dense(\n",
    "        tf.concat([next_state_inputs, target_actions], axis=-1), 32, activation=tf.nn.relu, trainable=False)\n",
    "    target_q_values = tf.squeeze(tf.layers.dense(hidden, 1, activation=None, trainable=False), axis=-1)\n",
    "\n",
    "critic_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='critic')\n",
    "critic_target_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='target_critic')\n",
    "update_critic_target_op = [tf.assign(ctp, (1 - TAU) * ctp + TAU * cp) for ctp, cp in zip(critic_target_params, critic_params)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('optimization'):\n",
    "    critic_loss = tf.reduce_mean(tf.squared_difference(reward_inputs + GAMMA * target_q_values, q_values))\n",
    "    critic_train_op = tf.train.AdamOptimizer(0.001).minimize(critic_loss)\n",
    "\n",
    "    action_grads = tf.gradients(q_values, actions)\n",
    "    actor_grads = tf.gradients(ys=actions, xs=actor_params, grad_ys=action_grads)\n",
    "    actor_train_op = tf.train.AdamOptimizer(-0.001).apply_gradients(zip(actor_grads, actor_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, noise_scale):\n",
    "    act = sess.run(actions, feed_dict={state_inputs: [state]})[0]\n",
    "    act = np.clip(np.random.normal(act, noise_scale), -action_space_bound, action_space_bound)\n",
    "    return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "replay_buffer = deque(maxlen=10000)\n",
    "noise_scale = 3.0\n",
    "\n",
    "total_rewards = []\n",
    "for episode in range(201):\n",
    "    total_rewards.append(0.0)\n",
    "    state = env.reset()\n",
    "    for _ in range(MAX_EP_STEPS):\n",
    "        act = choose_action(state, noise_scale)\n",
    "        next_state, r, done, _ = env.step(act)\n",
    "        replay_buffer.append({'s':state, 'r':r, 'next_s':next_state})\n",
    "        state = next_state\n",
    "        total_rewards[-1] += r\n",
    "\n",
    "        if len(replay_buffer) > BATCH_SIZE:\n",
    "            noise_scale *= .9999\n",
    "            mini_batch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "            fd = {\n",
    "                state_inputs: [rb['s'] for rb in mini_batch],\n",
    "                reward_inputs: [rb['r'] for rb in mini_batch],\n",
    "                next_state_inputs: [rb['next_s'] for rb in mini_batch], \n",
    "            }\n",
    "            sess.run(critic_train_op, fd)\n",
    "            sess.run(update_critic_target_op, fd)\n",
    "            sess.run(actor_train_op, fd)\n",
    "            sess.run(update_actor_target_op, fd)\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        print(\"Episode: {} | Mean Reward of 10 turns is: {:.2f} | noise_scale is: {:.5f}\".format(\n",
    "            episode,\n",
    "            np.mean(total_rewards[-10:]),\n",
    "            noise_scale\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
