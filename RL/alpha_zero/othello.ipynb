{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8065721d-a9a2-4d53-8c3a-6ddfa21bd167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import boardgame2\n",
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from alpha_zero import ReplayBuffer\n",
    "from alpha_zero import play_game\n",
    "from alpha_zero import run_mcts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a9d94a-be38-4b5d-8262-dd29b3ed254d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZeroConfig(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.num_sampling_moves = 15\n",
    "        self.max_moves = 36\n",
    "        self.num_simulations = 50\n",
    "\n",
    "        self.root_dirichlet_alpha = 0.15\n",
    "        self.root_exploration_fraction = 0.25\n",
    "\n",
    "        self.pb_c_base = 19652\n",
    "        self.pb_c_init = 1.25\n",
    "\n",
    "        self.training_steps = 2500\n",
    "        self.window_size = 1000\n",
    "        self.batch_size = 256\n",
    "\n",
    "        self.learning_rate = 0.001\n",
    "        self.weight_decay = 0.01\n",
    "\n",
    "        self.base_dir = '/tmp/othello_6x6/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3254166-7568-4d50-91a9-7e49fb864848",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game(object):\n",
    "    def __init__(self, history=None):\n",
    "        self.board_size = 6\n",
    "        self.env = gym.make('Reversi-v0', board_shape=[self.board_size, self.board_size])\n",
    "        self.env.reset()\n",
    "\n",
    "        self.history = history or [(self.env.board, self.env.player, 0.0, False)]\n",
    "        self.env.board, self.env.player, _, _ = self.history[-1]\n",
    "\n",
    "        self.child_visits = []\n",
    "        self.num_actions = self.board_size ** 2\n",
    "\n",
    "    def terminal(self):\n",
    "        _, _, _, done = self.history[-1]\n",
    "        return done\n",
    "\n",
    "    def terminal_value(self):\n",
    "        _, _, reward, _ = self.history[-1]\n",
    "        return reward\n",
    "\n",
    "    def legal_actions(self):\n",
    "        board, player, _, _ = self.history[-1]\n",
    "        valid_moves = np.where(\n",
    "            self.env.get_valid((board, player)).flatten() == 1)[0]\n",
    "        return valid_moves\n",
    "\n",
    "    def clone(self):\n",
    "        return Game(copy.copy(self.history))\n",
    "\n",
    "    def apply(self, action):\n",
    "        (board, player), reward, done, _ = self.env.step([\n",
    "            action // self.board_size, action % self.board_size])\n",
    "        self.history.append([board, player, float(reward), done])\n",
    "\n",
    "    def step(self, row, col):\n",
    "        (board, player), reward, done, _ = self.env.step([row, col])\n",
    "        self.history.append([board, player, float(reward), done])\n",
    "\n",
    "    def store_search_statistics(self, root):\n",
    "        sum_visits = sum(child.visit_count for child in root.children.values())\n",
    "        visit_dist = [\n",
    "            root.children[a].visit_count / sum_visits if a in root.children else 0\n",
    "            for a in range(self.num_actions)\n",
    "        ]\n",
    "        visit_dist = np.asarray(visit_dist).reshape([self.board_size, self.board_size])\n",
    "        self.child_visits.append(visit_dist)\n",
    "\n",
    "    def make_image(self, state_index: int):\n",
    "        board, player, _, _ = self.history[state_index]\n",
    "        return board, player\n",
    "\n",
    "    def make_target(self, state_index: int):\n",
    "        _, player, _, _ = self.history[state_index]\n",
    "        return self.terminal_value(), self.child_visits[state_index]\n",
    "\n",
    "    def to_play(self):\n",
    "        _, player, _, _ = self.history[-1]\n",
    "        return player\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1a21f1-f227-467d-9317-c4a3ddf22ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.representation = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Conv2D(128, 3, padding=\"same\", use_bias=False, name='conv1'),\n",
    "            tf.keras.layers.BatchNormalization(name='batch_norm_1'),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(128, 3, padding=\"valid\", use_bias=False, name='conv2'),\n",
    "            tf.keras.layers.BatchNormalization(name='batch_norm_2'),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(256, use_bias=False, name='fc1'),\n",
    "            tf.keras.layers.BatchNormalization(name='batch_norm_3'),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(128, use_bias=False, name='fc2'),\n",
    "            tf.keras.layers.BatchNormalization(name='batch_norm_4'),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "        ], name='representation')\n",
    "        self.value = tf.keras.layers.Dense(1, activation='tanh', name='value')\n",
    "        self.policy = tf.keras.layers.Dense(36, name='policy')\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        outputs = self.representation(inputs, training=training)\n",
    "        values = tf.reshape(self.value(outputs), [-1])\n",
    "        policy_logits = self.policy(outputs)\n",
    "\n",
    "        return values, policy_logits\n",
    "\n",
    "    @tf.function(input_signature=(\n",
    "        tf.TensorSpec(shape=[None, 6, 6], dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.float32),\n",
    "    ))\n",
    "    def inference(self, boards, players):\n",
    "        inputs = tf.stack([\n",
    "            boards,\n",
    "            tf.ones_like(boards) * players[:, tf.newaxis, tf.newaxis]\n",
    "        ], axis=3)\n",
    "\n",
    "        values, policy_logits = self.call(inputs, training=False)\n",
    "        return values, policy_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa584f68-f119-4772-b29d-19f9f9cb95dc",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6be21a5-163d-48f2-8d5d-481ddf54600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_data(dataset):\n",
    "    inputs = []\n",
    "    value_targets = []\n",
    "    policy_targets = []\n",
    "\n",
    "    for (board, player), (value, policy) in dataset:\n",
    "        inputs.append(\n",
    "            np.stack([board, np.ones_like(board) * player], axis=2))\n",
    "        value_targets.append(value)\n",
    "        policy_targets.append(policy.flatten())\n",
    "\n",
    "    return np.array(inputs, dtype=np.float32), np.array(value_targets, dtype=np.float32), np.array(policy_targets, dtype=np.float32)\n",
    "\n",
    "@tf.function\n",
    "def update_weights(batch_inputs, batch_value_targets, batch_policy_targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        values, policy_logits = network(batch_inputs, training=True)\n",
    "        value_loss = tf.losses.mean_squared_error(values, batch_value_targets)\n",
    "        policy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=policy_logits, labels=batch_policy_targets))\n",
    "\n",
    "        loss = value_loss + policy_loss\n",
    "        for v in network.trainable_variables:\n",
    "            if 'bias' not in v.name and 'batch_norm' not in v.name:\n",
    "                loss += config.weight_decay * tf.nn.l2_loss(v)\n",
    "\n",
    "    variables = network.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss, value_loss, policy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7224ff33-4bfb-438e-82e5-5cd7fedda85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AlphaZeroConfig()\n",
    "replay_buffer = ReplayBuffer(config)\n",
    "network = Network()\n",
    "optimizer = tf.keras.optimizers.Adam(config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b5f73e-501b-4d88-8076-d68f517aed8c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(config.batch_size):\n",
    "    game = play_game(config, Game(), network)\n",
    "    replay_buffer.save_game(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eadda03-7a55-4146-baa0-6afddc987b65",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_metric = tf.keras.metrics.Mean(name='loss')\n",
    "value_metric = tf.keras.metrics.Mean(name='value_loss')\n",
    "policy_metric = tf.keras.metrics.Mean(name='policy_loss')\n",
    "start = time.time()\n",
    "\n",
    "for i in range(config.training_steps):\n",
    "    game = play_game(config, Game(), network)\n",
    "    replay_buffer.save_game(game)\n",
    "\n",
    "    batch_dateset = replay_buffer.sample_batch()\n",
    "    batch_inputs, batch_value_targets, batch_policy_targets = create_train_data(batch_dateset)\n",
    "    loss, value_loss, policy_loss = update_weights(\n",
    "        batch_inputs, batch_value_targets, batch_policy_targets)\n",
    "\n",
    "    loss_metric(loss)\n",
    "    value_metric(value_loss)\n",
    "    policy_metric(policy_loss)\n",
    "\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print('{}/{}, Loss: {:.4f}, Valus Loss: {:.4f}, Policy Loss: {:.4f}, Elapsed Time: {:.2f}'.format(\n",
    "            i + 1,\n",
    "            config.training_steps,\n",
    "            loss_metric.result().numpy(),\n",
    "            value_metric.result().numpy(),\n",
    "            policy_metric.result().numpy(),\n",
    "            time.time() - start\n",
    "        ))\n",
    "\n",
    "        loss_metric.reset_states()\n",
    "        value_metric.reset_states()\n",
    "        policy_metric.reset_states()\n",
    "        start = time.time()\n",
    "\n",
    "network.save_weights(config.base_dir + 'weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d233735-088f-4fff-9fc3-4ada22103deb",
   "metadata": {},
   "source": [
    "# Play Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a91313-0891-4a37-85ad-c0b42a117151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_board(board, player, legal_actions, ai_moves):\n",
    "    board = copy.copy(board)\n",
    "    num_row, num_col = board.shape\n",
    "\n",
    "    legal_actions = [(a // num_col, a % num_col) for a in legal_actions] # show the move candidates\n",
    "    ai_moves = [(a // num_col, a % num_col) for a in ai_moves] # show the AI moves\n",
    "\n",
    "    plt.rcParams['axes.facecolor'] = 'g'\n",
    "    plt.rcParams['text.color'] = 'k'\n",
    "    plt.rcParams['xtick.color'] = 'k'\n",
    "    plt.rcParams['ytick.color'] = 'k'\n",
    "    plt.figure(figsize=(4, 4), facecolor='w')\n",
    "    plt.subplot(111)\n",
    "    plt.title('●:○ = {}:{}'.format(\n",
    "        np.sum(board == 1),\n",
    "        np.sum(board == -1)\n",
    "    ), y=-0.14, fontsize=12)\n",
    "\n",
    "    for y_pos in range(num_row):\n",
    "        plt.axhline(y_pos-.5, color='k', lw=2)\n",
    "        for x_pos in range(num_col):\n",
    "            plt.axvline(x_pos-.5, color='k', lw=2)\n",
    "            if board[y_pos, x_pos] == 1:\n",
    "                plt.plot(x_pos, y_pos, 'o', color='k', ms=30)\n",
    "            elif board[y_pos, x_pos] == -1:\n",
    "                plt.plot(x_pos, y_pos, 'o', color='w', ms=30)\n",
    "\n",
    "            if (y_pos, x_pos) in legal_actions:\n",
    "                plt.plot(\n",
    "                    x_pos,\n",
    "                    y_pos,\n",
    "                    'o',\n",
    "                    color='k' if player == 1 else 'w',\n",
    "                    ms=30,\n",
    "                    markerfacecolor='none'\n",
    "                )\n",
    "\n",
    "            if (y_pos, x_pos) in ai_moves:\n",
    "                plt.plot(x_pos, y_pos, '^', color='r', ms=10)\n",
    "\n",
    "    plt.xlim([-.5, num_col-.5])\n",
    "    plt.ylim([-.5, num_row-.5])\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.gca().set_yticks(range(num_row))\n",
    "    plt.gca().set_yticks(range(num_col))\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.gca().xaxis.set_ticks_position('top')\n",
    "    plt.tick_params(length=0)\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234d266f-d605-461f-80e7-99c8d9abc425",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AlphaZeroConfig()\n",
    "network = Network()\n",
    "network.load_weights(config.base_dir + 'weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1553f6fd-1ef6-4672-a23b-0a949717f591",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Game()\n",
    "show_board(game.env.board, game.env.player, game.legal_actions(), [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535bf6b7-19ff-4c33-a1fa-28d74e35ae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.step(4, 5)\n",
    "\n",
    "_, node = run_mcts(config, game, network)\n",
    "best_action = max(\n",
    "    node.children.keys(),\n",
    "    key=lambda k: node.children[k].visit_count\n",
    ")\n",
    "game.apply(best_action)\n",
    "\n",
    "show_board(game.env.board, game.env.player, game.legal_actions(), [best_action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c566f6c-e9f6-4994-8887-550e6966552d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
