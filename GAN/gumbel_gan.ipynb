{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRITIC_ITERS = 1\n",
    "BATCH_SIZE = 64\n",
    "ITERS = 100000\n",
    "TEMPERATURE = 0.001\n",
    "EPS = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_probs = {\n",
    "    0: 0.1,\n",
    "    1: 0.2,\n",
    "    2: 0.3,\n",
    "    3: 0.25,\n",
    "    4: 0.15\n",
    "}\n",
    "\n",
    "def create_real_data():\n",
    "    samples = np.random.choice(\n",
    "        list(real_probs.keys()),\n",
    "        p=list(real_probs.values()),\n",
    "        size=BATCH_SIZE\n",
    "    )\n",
    "    return np.identity(len(real_probs))[samples]\n",
    "\n",
    "def Generator(n_samples):\n",
    "    with tf.variable_scope('Generator'):\n",
    "        logits = tf.get_variable('logits', initializer=tf.ones([len(real_probs)]))\n",
    "        gumbel_dist = tfp.distributions.RelaxedOneHotCategorical(TEMPERATURE, logits=logits)\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        outputs = gumbel_dist.sample(n_samples)\n",
    "        return outputs, probs\n",
    "\n",
    "def Discriminator(inputs):\n",
    "    with tf.variable_scope('Discriminator', reuse=tf.AUTO_REUSE):\n",
    "        return tf.layers.dense(inputs, units=1, activation=tf.nn.sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data = tf.placeholder(tf.float32, shape=[None, 5])\n",
    "\n",
    "fake_data, fake_probs = Generator(BATCH_SIZE)\n",
    "disc_real = Discriminator(real_data)\n",
    "disc_fake = Discriminator(fake_data)\n",
    "\n",
    "disc_loss = -tf.reduce_mean(tf.log(disc_real + EPS) + tf.log(1.0 - disc_fake + EPS))\n",
    "gen_loss = -tf.reduce_mean(tf.log(disc_fake + EPS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_params = tf.trainable_variables('Discriminator')\n",
    "gen_params = tf.trainable_variables('Generator')\n",
    "\n",
    "disc_train_op = tf.train.AdamOptimizer(\n",
    "    learning_rate=1e-3,\n",
    ").minimize(disc_loss, var_list=disc_params)\n",
    "\n",
    "gen_train_op = tf.train.AdamOptimizer(\n",
    "    learning_rate=1e-3,\n",
    ").minimize(gen_loss, var_list=gen_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(ITERS+1):\n",
    "    for _ in range(CRITIC_ITERS):\n",
    "        fd = {real_data: create_real_data()}\n",
    "        sess.run(disc_train_op, fd)\n",
    "\n",
    "    sess.run(gen_train_op)\n",
    "\n",
    "    if i % 10000 == 0:\n",
    "        print(i, ' : {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}'.format(*sess.run(fake_probs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
