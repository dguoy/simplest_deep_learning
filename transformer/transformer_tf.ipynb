{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_id = {\n",
    "    '<PAD>': 0,\n",
    "    '<BOS>': 1,\n",
    "    '<EOS>': 2,\n",
    "    '0': 3,\n",
    "    '1': 4,\n",
    "    '2': 5,\n",
    "    '3': 6,\n",
    "    '4': 7,\n",
    "    '5': 8,\n",
    "    '6': 9,\n",
    "    '7': 10,\n",
    "    '8': 11,\n",
    "    '9': 12,\n",
    "    '+': 13,\n",
    "    '-': 14,\n",
    "}\n",
    "\n",
    "id_to_char = {\n",
    "    0: '<PAD>',\n",
    "    1: '<BOS>',\n",
    "    2: '<EOS>',\n",
    "    3: '0',\n",
    "    4: '1',\n",
    "    5: '2',\n",
    "    6: '3',\n",
    "    7: '4',\n",
    "    8: '5',\n",
    "    9: '6',\n",
    "    10: '7',\n",
    "    11: '8',\n",
    "    12: '9',\n",
    "    13: '+',\n",
    "    14: '-',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_list_to_sequence(sequence):\n",
    "    return ''.join([id_to_char.get(i) for i in sequence])\n",
    "\n",
    "def sequence_to_id_list(sequence):\n",
    "    return [char_to_id.get(c) for c in sequence]\n",
    "\n",
    "def create_dataset(size, num_digit=5, ops=['+', '-']):\n",
    "    source_sequences = []\n",
    "    target_sequences = []\n",
    "\n",
    "    for _ in range(size):\n",
    "        a = random.randint(0, 10**num_digit)\n",
    "        b = random.randint(0, 10**num_digit)\n",
    "        op = random.choice(ops)\n",
    "\n",
    "        if op == '+':\n",
    "            source_tokens = '{}+{}'.format(a, b)\n",
    "            target_tokens = '{}'.format(a + b)\n",
    "        elif op == '-':\n",
    "            source_tokens = '{}-{}'.format(a, b)\n",
    "            target_tokens = '{}'.format(a - b)\n",
    "\n",
    "        source_sequences.append(source_tokens)\n",
    "        target_sequences.append(target_tokens)\n",
    "\n",
    "    return source_sequences, target_sequences\n",
    "\n",
    "def tokenize(sequences, bos=False, eos=False):\n",
    "    tensor = [\n",
    "        ([char_to_id['<BOS>']] if bos else []) + \\\n",
    "        sequence_to_id_list(s) + \\\n",
    "        ([char_to_id['<EOS>']] if eos else [])\n",
    "        for s in sequences\n",
    "    ]\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_num = 25600\n",
    "batch_size = 256\n",
    "\n",
    "train_source_sequences, train_target_sequences = create_dataset(train_data_num)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    tokenize(train_source_sequences),\n",
    "    tokenize(train_target_sequences, bos=True),\n",
    "    tokenize(train_target_sequences, eos=True)\n",
    ")).shuffle(len(train_source_sequences))\n",
    "train_dataset = train_dataset.batch(batch_size, drop_remainder=True).repeat().prefetch(8)\n",
    "train_data_iter = iter(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(train_source_sequences[i], '=', train_target_sequences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(char_to_id)\n",
    "num_blocks = 2\n",
    "num_hidden_size = 128\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "\n",
    "num_epochs = 100\n",
    "num_batches = train_data_num // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG_INF = -1e9\n",
    "\n",
    "\n",
    "def gelu(x):\n",
    "    cdf = 0.5 * (1.0 + tf.tanh(0.7978845608 * (x + 0.044715 * tf.pow(x, 3))))\n",
    "    return x * cdf\n",
    "\n",
    "\n",
    "def get_position_encoding(length, hidden_size, dtype=tf.float32):\n",
    "    position = tf.cast(tf.range(length), dtype)\n",
    "    timescale = tf.cast(tf.range(hidden_size // 2), dtype)\n",
    "\n",
    "    angle_rates = 1.0 / tf.pow(10000.0, (2 * timescale) / tf.cast(hidden_size, dtype))\n",
    "    angle_rads = position[:, tf.newaxis] * angle_rates[tf.newaxis, :]\n",
    "\n",
    "    position_encoding = tf.stack([\n",
    "        tf.sin(angle_rads), tf.cos(angle_rads)\n",
    "    ], axis=2)\n",
    "    position_encoding = tf.reshape(position_encoding, [length, hidden_size])\n",
    "\n",
    "    return position_encoding\n",
    "\n",
    "\n",
    "def get_padding_bias(x):\n",
    "    padding = tf.cast(tf.equal(x, 0), tf.float32)\n",
    "    attention_bias = padding * NEG_INF\n",
    "    attention_bias = tf.expand_dims(\n",
    "        tf.expand_dims(attention_bias, axis=1), axis=1)\n",
    "    return attention_bias\n",
    "\n",
    "\n",
    "def get_decoder_self_attention_bias(length):\n",
    "    valid_locs = tf.linalg.band_part(tf.ones([length, length]), -1, 0)\n",
    "    valid_locs = tf.reshape(valid_locs, [1, 1, length, length])\n",
    "    decoder_bias = NEG_INF * (1.0 - valid_locs)\n",
    "    return decoder_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingSharedWeights(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(EmbeddingSharedWeights, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_weights = self.add_weight(\n",
    "            name='embedding_weights',\n",
    "            shape=[self.vocab_size, self.hidden_size],\n",
    "            initializer=tf.random_normal_initializer(mean=0., stddev=self.hidden_size**-0.5)\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, mode='embedding'):\n",
    "        if mode == 'embedding':\n",
    "            return self._embedding(inputs)\n",
    "        elif mode == 'linear':\n",
    "            return self._linear(inputs)\n",
    "\n",
    "    def _embedding(self, inputs):\n",
    "        embeddings = tf.gather(self.embedding_weights, inputs)\n",
    "        mask = tf.cast(tf.not_equal(inputs, 0), embeddings.dtype)\n",
    "        embeddings *= tf.expand_dims(mask, -1)\n",
    "        embeddings *= self.hidden_size ** 0.5\n",
    "        return embeddings\n",
    "\n",
    "    def _linear(self, inputs):\n",
    "        outputs = tf.matmul(inputs, self.embedding_weights, transpose_b=True)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, hidden_size, filter_size, dropout_rate) -> None:\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.filter_dense_layer = tf.keras.layers.Dense(\n",
    "            filter_size, use_bias=True, activation=gelu, name='filter_layer')\n",
    "        self.output_dense_layer = tf.keras.layers.Dense(\n",
    "            hidden_size, use_bias=True, name='output_layer')\n",
    "        self.dropout_layer = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, training):\n",
    "        output = self.filter_dense_layer(x)\n",
    "        output = self.dropout_layer(output, training=training)\n",
    "        output = self.output_dense_layer(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, hidden_size, num_heads, dropout_rate):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = hidden_size // num_heads\n",
    "\n",
    "        self.q_dense_layer = tf.keras.layers.Dense(hidden_size, use_bias=False, name='q')\n",
    "        self.k_dense_layer = tf.keras.layers.Dense(hidden_size, use_bias=False, name='k')\n",
    "        self.v_dense_layer = tf.keras.layers.Dense(hidden_size, use_bias=False, name='v')\n",
    "        self.output_dense_layer = tf.keras.layers.Dense(\n",
    "            hidden_size, use_bias=False, name='output_transform')\n",
    "        self.dropout_layer = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, y, bias, training):\n",
    "        q = self.q_dense_layer(x)\n",
    "        k = self.k_dense_layer(y)\n",
    "        v = self.v_dense_layer(y)\n",
    "\n",
    "        q = self._split_heads(q)\n",
    "        k = self._split_heads(k)\n",
    "        v = self._split_heads(v)\n",
    "\n",
    "        q = q * self.depth ** -0.5\n",
    "\n",
    "        logits = tf.matmul(q, k, transpose_b=True)\n",
    "        logits += bias\n",
    "        weights = tf.nn.softmax(logits, name='attention_weights')\n",
    "        weights = self.dropout_layer(weights, training=training)\n",
    "\n",
    "        attention_output = tf.matmul(weights, v)\n",
    "        attention_output = self._combine_heads(attention_output)\n",
    "        attention_output = self.output_dense_layer(attention_output)\n",
    "\n",
    "        return attention_output\n",
    "\n",
    "    def _split_heads(self, x):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        length = tf.shape(x)[1]\n",
    "        x = tf.reshape(x, [batch_size, length, self.num_heads, self.depth])\n",
    "        return tf.transpose(x, [0, 2, 1, 3])\n",
    "\n",
    "    def _combine_heads(self, x):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        length = tf.shape(x)[2]\n",
    "        x = tf.transpose(x, [0, 2, 1, 3])\n",
    "        return tf.reshape(x, [batch_size, length, self.hidden_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(Attention):\n",
    "    def call(self, x, bias, training):\n",
    "        return super(SelfAttention, self).call(x, x, bias, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrePostProcessingWrapper(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, layer, dropout_rate):\n",
    "        super(PrePostProcessingWrapper, self).__init__()\n",
    "        self.layer = layer\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout_layer = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, *args, **kwargs):\n",
    "        y = self.layer_norm(x)\n",
    "        y = self.layer(y, *args, **kwargs)\n",
    "        y = self.dropout_layer(y, training=kwargs['training'])\n",
    "\n",
    "        return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderStack(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, num_blocks, hidden_size, num_heads, dropout_rate):\n",
    "        super(EncoderStack, self).__init__()\n",
    "        self.layers = []\n",
    "        for _ in range(num_blocks):\n",
    "            self_attention_layer = SelfAttention(hidden_size, num_heads, dropout_rate)\n",
    "            feed_forward_network = FeedForwardNetwork(hidden_size, hidden_size * 4, dropout_rate)\n",
    "\n",
    "            self.layers.append([\n",
    "                PrePostProcessingWrapper(self_attention_layer, dropout_rate),\n",
    "                PrePostProcessingWrapper(feed_forward_network, dropout_rate)\n",
    "            ])\n",
    "\n",
    "        self.output_normalization = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, encoder_inputs, attention_bias, training):\n",
    "        for n, layer in enumerate(self.layers):\n",
    "            self_attention_layer = layer[0]\n",
    "            feed_forward_network = layer[1]\n",
    "\n",
    "            encoder_inputs = self_attention_layer(\n",
    "                encoder_inputs, attention_bias, training=training)\n",
    "            encoder_inputs = feed_forward_network(\n",
    "                encoder_inputs, training=training)\n",
    "\n",
    "        return self.output_normalization(encoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderStack(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, num_blocks, hidden_size, num_heads, dropout_rate):\n",
    "        super(DecoderStack, self).__init__()\n",
    "        self.layers = []\n",
    "        for _ in range(num_blocks):\n",
    "            self_attention_layer = SelfAttention(hidden_size, num_heads, dropout_rate)\n",
    "            enc_dec_attention_layer = Attention(hidden_size, num_heads, dropout_rate)\n",
    "            feed_forward_network = FeedForwardNetwork(hidden_size, hidden_size * 4, dropout_rate)\n",
    "\n",
    "            self.layers.append([\n",
    "                PrePostProcessingWrapper(self_attention_layer, dropout_rate),\n",
    "                PrePostProcessingWrapper(enc_dec_attention_layer, dropout_rate),\n",
    "                PrePostProcessingWrapper(feed_forward_network, dropout_rate)\n",
    "            ])\n",
    "\n",
    "        self.output_normalization = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        decoder_inputs,\n",
    "        encoder_outputs,\n",
    "        decoder_self_attention_bias,\n",
    "        attention_bias,\n",
    "        training\n",
    "    ):\n",
    "        for n, layer in enumerate(self.layers):\n",
    "            self_attention_layer = layer[0]\n",
    "            enc_dec_attention_layer = layer[1]\n",
    "            feed_forward_network = layer[2]\n",
    "\n",
    "            decoder_inputs = self_attention_layer(\n",
    "                decoder_inputs, decoder_self_attention_bias, training=training)\n",
    "            decoder_inputs = enc_dec_attention_layer(\n",
    "                decoder_inputs, encoder_outputs, attention_bias, training=training)\n",
    "            decoder_inputs = feed_forward_network(decoder_inputs, training=training)\n",
    "\n",
    "        return self.output_normalization(decoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        num_blocks,\n",
    "        hidden_size,\n",
    "        num_heads,\n",
    "        dropout_rate,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.embedding_softmax_layer = EmbeddingSharedWeights(vocab_size, hidden_size)\n",
    "        self.encoder_stack = EncoderStack(num_blocks, hidden_size, num_heads, dropout_rate)\n",
    "        self.decoder_stack = DecoderStack(num_blocks, hidden_size, num_heads, dropout_rate)\n",
    "\n",
    "        self.encoder_dropout_layer = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.decoder_dropout_layer = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, encoder_inputs, decoder_inputs, training):\n",
    "        attention_bias = get_padding_bias(encoder_inputs)\n",
    "        encoder_outputs = self.encode(encoder_inputs, attention_bias, training=training)\n",
    "        logits = self.decode(decoder_inputs, encoder_outputs, attention_bias, training=training)\n",
    "        return logits\n",
    "\n",
    "    def encode(self, inputs, attention_bias, training):\n",
    "        embedded_inputs = self.embedding_softmax_layer(inputs)\n",
    "\n",
    "        with tf.name_scope('add_pos_encoding'):\n",
    "            length = tf.shape(embedded_inputs)[1]\n",
    "            pos_encoding = get_position_encoding(length, self.hidden_size)\n",
    "            encoder_inputs = embedded_inputs + pos_encoding\n",
    "            encoder_inputs = self.encoder_dropout_layer(encoder_inputs, training=training)\n",
    "\n",
    "        return self.encoder_stack(encoder_inputs, attention_bias, training=training)\n",
    "\n",
    "    def decode(self, inputs, encoder_outputs, attention_bias, training):\n",
    "        embedded_inputs = self.embedding_softmax_layer(inputs)\n",
    "\n",
    "        with tf.name_scope('add_pos_encoding'):\n",
    "            length = tf.shape(embedded_inputs)[1]\n",
    "            pos_encoding = get_position_encoding(length, self.hidden_size)\n",
    "            decoder_inputs = embedded_inputs + pos_encoding\n",
    "            decoder_inputs = self.decoder_dropout_layer(decoder_inputs, training=training)\n",
    "\n",
    "        decoder_self_attention_bias = get_decoder_self_attention_bias(length)\n",
    "        decoder_outputs = self.decoder_stack(\n",
    "            decoder_inputs,\n",
    "            encoder_outputs,\n",
    "            decoder_self_attention_bias,\n",
    "            attention_bias,\n",
    "            training=training\n",
    "        )\n",
    "        logits = self.embedding_softmax_layer(decoder_outputs, mode='linear')\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    vocab_size,\n",
    "    num_blocks,\n",
    "    num_hidden_size,\n",
    "    num_heads,\n",
    "    dropout_rate,\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_metric = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy_metric = tf.keras.metrics.Mean(name='train_accuracy')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    targets_one_hot = tf.one_hot(real, vocab_size)\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=pred,\n",
    "        labels=targets_one_hot\n",
    "    )\n",
    "\n",
    "    weights = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    weights = tf.cast(weights, dtype=loss.dtype)\n",
    "\n",
    "    return tf.reduce_sum(loss * weights) / tf.reduce_sum(weights)\n",
    "\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    predicted_ids = tf.cast(tf.argmax(pred, axis=-1), tf.int32)\n",
    "    correct = tf.cast(tf.equal(predicted_ids, real), tf.float32)\n",
    "\n",
    "    weights = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    weights = tf.cast(weights, dtype=tf.float32)\n",
    "\n",
    "    return tf.reduce_sum(correct * weights) / tf.reduce_sum(weights)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(dataset_inputs):\n",
    "    encoder_inputs, decoder_inputs, decoder_targets = dataset_inputs\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = transformer(encoder_inputs, decoder_inputs, training=True)\n",
    "        loss = loss_function(decoder_targets, logits)\n",
    "\n",
    "    variables = transformer.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    train_loss_metric(loss)\n",
    "    accuracy = accuracy_function(decoder_targets, logits)\n",
    "    train_accuracy_metric(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "template = '{}/{} (epoch {}), Train Loss: {:.4f}, Train Accuracy: {:.4f}, Elapsed Time: {:.2f}'\n",
    "start = time.time()\n",
    "for e in range(num_epochs):\n",
    "    for i in range(num_batches):\n",
    "        batch_train_data = next(train_data_iter)\n",
    "        train_step(batch_train_data)\n",
    "\n",
    "        if (e * num_batches + i + 1) % 100 == 0:\n",
    "            print(template.format(\n",
    "                e * num_batches + i + 1,\n",
    "                num_epochs * num_batches,\n",
    "                e + 1,\n",
    "                train_loss_metric.result().numpy(),\n",
    "                train_accuracy_metric.result().numpy(),\n",
    "                time.time() - start\n",
    "            ))\n",
    "\n",
    "            train_loss_metric.reset_states()\n",
    "            train_accuracy_metric.reset_states()\n",
    "            start = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data_num = 10240\n",
    "valid_batch_size = 1024\n",
    "\n",
    "valid_source_sequences, valid_target_sequences = create_dataset(valid_data_num)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    tokenize(valid_source_sequences),\n",
    "    tokenize(valid_target_sequences, bos=True),\n",
    "    tokenize(valid_target_sequences, eos=True)\n",
    "))\n",
    "valid_dataset = valid_dataset.batch(valid_batch_size, drop_remainder=False).repeat().prefetch(8)\n",
    "valid_data_iter = iter(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loss_metric = tf.keras.metrics.Mean(name='valid_loss')\n",
    "valid_accuracy_metric = tf.keras.metrics.Mean(name='valid_accuracy')\n",
    "\n",
    "for _ in range(valid_data_num // valid_batch_size):\n",
    "    encoder_inputs, decoder_inputs, decoder_targets = next(valid_data_iter)\n",
    "    logits = transformer(encoder_inputs, decoder_inputs, training=False)\n",
    "    loss = loss_function(decoder_targets, logits)\n",
    "\n",
    "    valid_loss_metric(loss)\n",
    "    accuracy = accuracy_function(decoder_targets, logits)\n",
    "    valid_accuracy_metric(accuracy)\n",
    "\n",
    "print('Valid Loss: {:.4f}, Valid Accuracy: {:.4f}'.format(\n",
    "    valid_loss_metric.result().numpy(),\n",
    "    valid_accuracy_metric.result().numpy()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
